{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05c63495",
   "metadata": {},
   "source": [
    "# Exp 1 — Ring Line World with Rewards + Energy (LLM as Policy)\n",
    "\n",
    "Goal:\n",
    "- Build the *minimal* simulation environment on a **ring line** (periodic boundary).\n",
    "- Add **rewards** at locations (collected when landed on).\n",
    "- Add **energy** (movement costs energy; if 0 → no movement).\n",
    "- Use an **LLM policy** (Ollama) to choose actions.\n",
    "- Log trajectories + compute basic metrics for **behavioral strategy inference** (no self-report).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d006ea04",
   "metadata": {},
   "source": [
    "## Key Concept: LLM as a Policy\n",
    "\n",
    "A *policy* is a function:\n",
    "\n",
    "**observation → action**\n",
    "\n",
    "Here, the LLM is the policy:\n",
    "1. We build a structured observation (JSON).\n",
    "2. We send it to the LLM with strict instructions.\n",
    "3. The LLM outputs a structured action (JSON).\n",
    "4. We validate and apply it to update the world.\n",
    "\n",
    "Important:\n",
    "- The LLM never changes the world directly.\n",
    "- The simulator applies world rules deterministically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ea44ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Dict, Literal, Optional, Tuple\n",
    "\n",
    "import requests\n",
    "from pydantic import BaseModel, Field, ValidationError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574e651d",
   "metadata": {},
   "source": [
    "## 1) Experiment Configuration\n",
    "\n",
    "We keep all experiment constants in one place:\n",
    "- ring length `L`\n",
    "- horizon `T`\n",
    "- energy + movement cost\n",
    "- reward map\n",
    "- partial observability radius\n",
    "- Ollama model + decoding settings\n",
    "- output directory for logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575f44bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- World parameters ----\n",
    "L = 20                 # ring positions: 0..19\n",
    "T = 50                 # max steps\n",
    "START_X = 0\n",
    "START_ENERGY = 25\n",
    "MOVE_COST = 1\n",
    "\n",
    "# reward locations: position -> value\n",
    "REWARDS_INIT = {3: 5.0, 9: 10.0, 14: 7.0}\n",
    "\n",
    "# partial observability: rewards visible within VIS_RADIUS ring-distance\n",
    "VIS_RADIUS = 3\n",
    "\n",
    "# ---- LLM (Ollama OpenAI-compatible) ----\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
    "OLLAMA_MODEL = \"llama3.1\"\n",
    "TEMPERATURE = 0.2\n",
    "MAX_TOKENS = 120\n",
    "\n",
    "# ---- Output ----\n",
    "RUN_DIR = Path(\"runs/exp_1_ring_line_rewards\") / time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LOG_PATH = RUN_DIR / \"trajectory.jsonl\"\n",
    "METRICS_PATH = RUN_DIR / \"metrics.json\"\n",
    "CONFIG_PATH = RUN_DIR / \"config.json\"\n",
    "\n",
    "print(\"Run dir:\", RUN_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4758db",
   "metadata": {},
   "source": [
    "## 2) Schemas: Observation and Action\n",
    "\n",
    "Why schemas?\n",
    "- Make the simulator–policy interface explicit and stable.\n",
    "- Validate LLM outputs safely.\n",
    "- Make later FastAPI integration trivial.\n",
    "\n",
    "Observation includes:\n",
    "- time step `t`\n",
    "- ring length `L`\n",
    "- agent state (`x`, `energy`)\n",
    "- visible rewards (partial observability)\n",
    "\n",
    "Action is minimal:\n",
    "- one field: `type` ∈ {LEFT, RIGHT, WAIT}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c2948e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ActionType = Literal[\"LEFT\", \"RIGHT\", \"WAIT\"]\n",
    "\n",
    "class Observation(BaseModel):\n",
    "    t: int\n",
    "    L: int\n",
    "    x: int\n",
    "    energy: int\n",
    "    visible_rewards: Dict[int, float] = Field(default_factory=dict)\n",
    "\n",
    "class Action(BaseModel):\n",
    "    type: ActionType\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2270bebf",
   "metadata": {},
   "source": [
    "## 3) World Mechanics (Ring + Energy + Reward)\n",
    "\n",
    "Ring boundary means:\n",
    "- RIGHT from 19 wraps to 0\n",
    "- LEFT from 0 wraps to 19\n",
    "\n",
    "Energy rules:\n",
    "- LEFT/RIGHT cost MOVE_COST energy\n",
    "- WAIT costs 0\n",
    "- if energy is 0, movement is not possible (forced WAIT behavior)\n",
    "\n",
    "Reward rules:\n",
    "- if agent lands on a reward position, it collects that value\n",
    "- for simplicity, the reward is removed after collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cd4317",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ring_dist(a: int, b: int, L: int) -> int:\n",
    "    \"\"\"Shortest distance between positions a and b on a ring of length L.\"\"\"\n",
    "    d = abs(a - b)\n",
    "    return min(d, L - d)\n",
    "\n",
    "def observe(t: int, x: int, energy: int, rewards: Dict[int, float], L: int, vis_r: int) -> Observation:\n",
    "    \"\"\"Build the agent's partial observation: only rewards within visibility radius.\"\"\"\n",
    "    visible = {pos: val for pos, val in rewards.items() if ring_dist(x, pos, L) <= vis_r}\n",
    "    return Observation(t=t, L=L, x=x, energy=energy, visible_rewards=visible)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8d1919",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_action(x: int, energy: int, action: Action, L: int) -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Apply the action under energy constraints and ring dynamics.\n",
    "    Returns (new_x, new_energy).\n",
    "    \"\"\"\n",
    "    # If energy is depleted, movement is impossible\n",
    "    if energy <= 0:\n",
    "        return x, 0\n",
    "\n",
    "    # WAIT is always allowed\n",
    "    if action.type == \"WAIT\":\n",
    "        return x, energy\n",
    "\n",
    "    # Movement requires enough energy\n",
    "    if energy < MOVE_COST:\n",
    "        return x, energy  # cannot move\n",
    "\n",
    "    if action.type == \"LEFT\":\n",
    "        return (x - 1) % L, energy - MOVE_COST\n",
    "\n",
    "    if action.type == \"RIGHT\":\n",
    "        return (x + 1) % L, energy - MOVE_COST\n",
    "\n",
    "    # Defensive fallback (shouldn't happen with schema validation)\n",
    "    return x, energy\n",
    "\n",
    "def collect_reward(x: int, rewards: Dict[int, float]) -> float:\n",
    "    \"\"\"Collect reward at position x if present, and remove it from the world.\"\"\"\n",
    "    if x in rewards:\n",
    "        val = rewards[x]\n",
    "        del rewards[x]\n",
    "        return val\n",
    "    return 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b21fc5",
   "metadata": {},
   "source": [
    "## 4) Logging (JSON Lines)\n",
    "\n",
    "We log *one JSON object per step* (JSONL) so we can:\n",
    "- reconstruct the trajectory\n",
    "- compute behavioral metrics later\n",
    "- compare controlled conditions across runs\n",
    "\n",
    "We'll log:\n",
    "- observation\n",
    "- raw LLM output\n",
    "- parsed action\n",
    "- state transition + reward gained\n",
    "- remaining rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9946040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_event(path: Path, event: dict) -> None:\n",
    "    with path.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(event, ensure_ascii=False) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd1566f",
   "metadata": {},
   "source": [
    "## 5) LLM Policy\n",
    "\n",
    "The LLM policy is a black-box function:\n",
    "- input: Observation (serialized into JSON text)\n",
    "- output: Action JSON\n",
    "\n",
    "We enforce structure by:\n",
    "- strict system message: \"output ONLY JSON\"\n",
    "- validating output with Pydantic\n",
    "- fallback to WAIT on invalid outputs\n",
    "\n",
    "Note:\n",
    "- This is *not training*. The model is fixed.\n",
    "- Strategy inference comes from analyzing trajectories + metrics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8ee838",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_call(system: str, user: str) -> str:\n",
    "    \"\"\"\n",
    "    Call Ollama's OpenAI-compatible endpoint: /v1/chat/completions\n",
    "    Returns the assistant message content as a string.\n",
    "    \"\"\"\n",
    "    url = f\"{OLLAMA_BASE_URL}/chat/completions\"\n",
    "    payload = {\n",
    "        \"model\": OLLAMA_MODEL,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\", \"content\": user},\n",
    "        ],\n",
    "        \"temperature\": TEMPERATURE,\n",
    "        \"max_tokens\": MAX_TOKENS,\n",
    "    }\n",
    "    r = requests.post(url, json=payload, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    return r.json()[\"choices\"][0][\"message\"][\"content\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28017e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_policy(obs: Observation) -> Tuple[Action, str]:\n",
    "    \"\"\"\n",
    "    LLM policy:\n",
    "    - Build prompt\n",
    "    - Call LLM\n",
    "    - Parse JSON -> Action\n",
    "    - If invalid, fallback WAIT\n",
    "    Returns (Action, raw_llm_output)\n",
    "    \"\"\"\n",
    "    system = (\n",
    "        \"You are an agent on a ring line. \"\n",
    "        \"Output ONLY JSON with exactly this shape: {\\\"type\\\":\\\"LEFT\\\"|\\\"RIGHT\\\"|\\\"WAIT\\\"}. \"\n",
    "        \"No other keys. No extra text.\"\n",
    "    )\n",
    "\n",
    "    user = json.dumps({\n",
    "        \"observation\": obs.model_dump(),\n",
    "        \"allowed_actions\": [\"LEFT\", \"RIGHT\", \"WAIT\"],\n",
    "        \"note\": \"visible_rewards contains reward positions+values within view radius.\"\n",
    "    })\n",
    "\n",
    "    raw = llm_call(system, user).strip()\n",
    "\n",
    "    try:\n",
    "        action = Action.model_validate_json(raw)\n",
    "        return action, raw\n",
    "    except ValidationError:\n",
    "        # If the LLM output isn't valid JSON matching schema, we apply a safe fallback.\n",
    "        return Action(type=\"WAIT\"), raw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d855e248",
   "metadata": {},
   "source": [
    "## 6) Behavioral Metrics (Strategy Inference)\n",
    "\n",
    "We infer \"strategy\" from behavior, not explanations.\n",
    "\n",
    "Minimal metrics:\n",
    "- total reward collected\n",
    "- coverage (unique positions visited)\n",
    "- steps to first reward (if any)\n",
    "- action distribution conditioned on energy (high/mid/low)\n",
    "\n",
    "These already tell you:\n",
    "- exploration vs exploitation\n",
    "- energy-aware behavior (WAIT more when low energy?)\n",
    "- stability vs dithering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a747957",
   "metadata": {},
   "outputs": [],
   "source": [
    "def energy_bin(e: int) -> str:\n",
    "    if e >= 10: return \"high\"\n",
    "    if e >= 3:  return \"mid\"\n",
    "    return \"low\"\n",
    "\n",
    "def run(seed: int = 0) -> dict:\n",
    "    rng = random.Random(seed)  # reserved for later (if you randomize world)\n",
    "\n",
    "    x = START_X\n",
    "    energy = START_ENERGY\n",
    "    rewards = dict(REWARDS_INIT)\n",
    "\n",
    "    visited = {x}\n",
    "    total_reward = 0.0\n",
    "    first_reward_t: Optional[int] = None\n",
    "\n",
    "    action_counts_by_energy = {\n",
    "        \"high\": {\"LEFT\": 0, \"RIGHT\": 0, \"WAIT\": 0},\n",
    "        \"mid\":  {\"LEFT\": 0, \"RIGHT\": 0, \"WAIT\": 0},\n",
    "        \"low\":  {\"LEFT\": 0, \"RIGHT\": 0, \"WAIT\": 0},\n",
    "    }\n",
    "\n",
    "    # Save resolved config for reproducibility\n",
    "    CONFIG_PATH.write_text(json.dumps({\n",
    "        \"L\": L, \"T\": T,\n",
    "        \"START_X\": START_X, \"START_ENERGY\": START_ENERGY, \"MOVE_COST\": MOVE_COST,\n",
    "        \"REWARDS_INIT\": REWARDS_INIT, \"VIS_RADIUS\": VIS_RADIUS,\n",
    "        \"OLLAMA_MODEL\": OLLAMA_MODEL, \"TEMPERATURE\": TEMPERATURE, \"MAX_TOKENS\": MAX_TOKENS,\n",
    "        \"seed\": seed,\n",
    "    }, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    for t in range(T):\n",
    "        obs = observe(t, x, energy, rewards, L, VIS_RADIUS)\n",
    "\n",
    "        action, raw_output = llm_policy(obs)\n",
    "\n",
    "        # Track action distribution by energy BEFORE applying the action\n",
    "        action_counts_by_energy[energy_bin(energy)][action.type] += 1\n",
    "\n",
    "        # Apply dynamics\n",
    "        x2, energy2 = apply_action(x, energy, action, L)\n",
    "        gained = collect_reward(x2, rewards)\n",
    "\n",
    "        if gained > 0 and first_reward_t is None:\n",
    "            first_reward_t = t\n",
    "\n",
    "        visited.add(x2)\n",
    "        total_reward += gained\n",
    "\n",
    "        # Log this step\n",
    "        log_event(LOG_PATH, {\n",
    "            \"t\": t,\n",
    "            \"obs\": obs.model_dump(),\n",
    "            \"raw_llm_output\": raw_output,\n",
    "            \"action\": action.model_dump(),\n",
    "            \"x_before\": x,\n",
    "            \"energy_before\": energy,\n",
    "            \"x_after\": x2,\n",
    "            \"energy_after\": energy2,\n",
    "            \"reward_gained\": gained,\n",
    "            \"reward_total_so_far\": total_reward,\n",
    "            \"rewards_remaining\": dict(rewards),\n",
    "        })\n",
    "\n",
    "        # Update state\n",
    "        x, energy = x2, energy2\n",
    "\n",
    "        # Optional early stop if energy is depleted\n",
    "        if energy <= 0:\n",
    "            break\n",
    "\n",
    "    metrics = {\n",
    "        \"steps_run\": t + 1,\n",
    "        \"total_reward\": total_reward,\n",
    "        \"coverage_unique_positions\": len(visited),\n",
    "        \"first_reward_step\": first_reward_t,\n",
    "        \"action_counts_by_energy_bin\": action_counts_by_energy,\n",
    "        \"end_state\": {\"x\": x, \"energy\": energy, \"rewards_remaining\": rewards},\n",
    "    }\n",
    "\n",
    "    METRICS_PATH.write_text(json.dumps(metrics, indent=2), encoding=\"utf-8\")\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd39009e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = run(seed=0)\n",
    "print(\"Run dir:\", RUN_DIR)\n",
    "print(json.dumps(metrics, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfcf221",
   "metadata": {},
   "source": [
    "## 7) Inspecting Results\n",
    "\n",
    "Open these files:\n",
    "- `trajectory.jsonl`: one record per step (great for debugging + later analysis)\n",
    "- `metrics.json`: summary metrics for quick comparisons\n",
    "\n",
    "First questions:\n",
    "- Did total_reward increase?\n",
    "- How quickly did it find the first reward?\n",
    "- Does it WAIT more when energy is low?\n",
    "- Does it explore (coverage), or just oscillate?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dedc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show last 5 steps from the JSONL log (quick sanity check)\n",
    "lines = LOG_PATH.read_text(encoding=\"utf-8\").strip().splitlines()\n",
    "for row in lines[-5:]:\n",
    "    print(row)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
